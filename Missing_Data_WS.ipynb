{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PgJyTn-Sk-0XSXu88FqQeW8xqyHegoRw","timestamp":1668385249689},{"file_id":"1CY7oXxt0DpIOVKg3FGgTcr1lWf3_5mX8","timestamp":1667774074241}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vh-HecfbzuOU"},"source":["# **CIS 520: Machine Learning**\n","\n","## **Missing Data** \n","\n","\n","- **Content Creator:** Kenneth Shinn, Siyun Hu\n","- **Content Reviewers:** Aditya Pratap Singh\n","- **Objectives:** This worksheet will work through an example of missing data imputation using both means and regression. Here, we are going to compare the performance of those two missing data imputation techniques from lecture.\n"]},{"cell_type":"markdown","source":["# Initialize Penn Grader"],"metadata":{"id":"whC9tPOELt-4"}},{"cell_type":"code","source":["%%capture\n","!pip install penngrader"],"metadata":{"id":"NF2jR4SaLyHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","import os "],"metadata":{"id":"qK3Il7pyL0fu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For autograder only, do not modify this cell. \n","# True for Google Colab, False for autograder\n","NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n","if NOTEBOOK:\n","    print(\"[INFO, OK] Google Colab.\")\n","else:\n","    print(\"[INFO, OK] Autograder.\")\n","    sys.exit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_JfkKLmCL3vq","outputId":"c80a80aa-2bc7-4c3a-ced9-4a77d2197bd7","executionInfo":{"status":"ok","timestamp":1668385954839,"user_tz":300,"elapsed":6,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO, OK] Google Colab.\n"]}]},{"cell_type":"code","source":["#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n","#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n","STUDENT_ID = 57931095 # YOUR PENN-ID GOES HERE AS AN INTEGER#"],"metadata":{"id":"bHsDp_wBL6OS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import penngrader.grader\n","\n","grader = penngrader.grader.PennGrader(homework_id = 'CIS_5200_202230_HW_Missing_Data_WS', student_id = STUDENT_ID)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7Se0yYgL9QH","outputId":"83eb8039-2433-4641-d06e-ab6ac25309d1","executionInfo":{"status":"ok","timestamp":1668385955006,"user_tz":300,"elapsed":170,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PennGrader initialized with Student ID: 57931095\n","\n","Make sure this correct or we will not be able to store your grade\n"]}]},{"cell_type":"code","source":["# A helper function for grading utils\n","def grader_serialize(obj):        # A helper function\n","    '''Dill serializes Python object into a UTF-8 string'''\n","    byte_serialized = dill.dumps(obj, recurse = True)\n","    return base64.b64encode(byte_serialized).decode(\"utf-8\")"],"metadata":{"id":"2cxFX8_PMIgz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7Cvrn9I0Lqu"},"source":["# **Data Preparation**\n","\n","We will split up a data set into testing and training data. The training data will have data elements randomly dropped, and we will use the two strategies to impute the missing data. NOTE: this worksheet will have data missing at random. Then, we will train regression models on each of the imputed training sets, and see how they perform with the held out testing set!"]},{"cell_type":"code","metadata":{"id":"vZEzSotV1a_b"},"source":["import numpy as np\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import SimpleImputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","from random import random\n","from random import seed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lwmz-i3e51PH"},"source":["Let's create a dataset using sklearn's make_regression function. This function randomly generates a data set for a regression problem. "]},{"cell_type":"code","metadata":{"id":"v1iUb3yb50o-"},"source":["X, y = make_regression(n_samples = 100000, n_features = 4)\n","X_missing = X.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6rPcazV7llT"},"source":["Now, let's randomly drop values from the data set. These missing values will be replaced with np.nan."]},{"cell_type":"code","metadata":{"id":"vVkHm0oS6dqN"},"source":["seed(1)\n","for i in range(len(X)):\n","    if random() < .3:\n","        X_missing[i][(int) (random() * 4)] = np.nan\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTPTP2dOhWzX"},"source":["# split the dataset into training and test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","X_train_missing, X_test_missing, y_train, y_test = train_test_split(X_missing, y, test_size=0.3, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ouFZG25Z844I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668385999490,"user_tz":300,"elapsed":167,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"614151d6-3553-4ffb-f75e-0343400b0a12"},"source":["# make sure everything looks good!\n","print(X_train_missing[:10])\n","print(X_train[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.02322197  0.43696322  0.16850924  0.35625915]\n"," [-2.02072205         nan -0.85961145 -0.56427015]\n"," [ 0.66788048  0.44718598  0.91242955 -1.12755079]\n"," [ 1.30721872 -1.31701916  0.15915672 -0.67072238]\n"," [ 0.9207756  -0.53624338  1.58523065  1.49596327]\n"," [-1.17671915 -0.7447005  -0.08264724 -1.01993779]\n"," [ 0.95636031  0.15601228  1.52141585 -0.81046941]\n"," [ 0.06524279  1.77406856  0.14961796  0.98130446]\n"," [ 1.14780442  1.49477241 -0.29783578 -1.0252665 ]\n"," [        nan -0.79242102 -0.69546528  1.66362747]]\n","[[-0.02322197  0.43696322  0.16850924  0.35625915]\n"," [-2.02072205 -1.52540395 -0.85961145 -0.56427015]\n"," [ 0.66788048  0.44718598  0.91242955 -1.12755079]\n"," [ 1.30721872 -1.31701916  0.15915672 -0.67072238]\n"," [ 0.9207756  -0.53624338  1.58523065  1.49596327]\n"," [-1.17671915 -0.7447005  -0.08264724 -1.01993779]\n"," [ 0.95636031  0.15601228  1.52141585 -0.81046941]\n"," [ 0.06524279  1.77406856  0.14961796  0.98130446]\n"," [ 1.14780442  1.49477241 -0.29783578 -1.0252665 ]\n"," [-0.80292517 -0.79242102 -0.69546528  1.66362747]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"2LVtSuMM9rxK"},"source":["## **Simple Mean Based Imputation**\n","\n","Let's perform a mean based imputation on the training set and test set respectively. Remember that we cannot use the data in the training set to impute the missing value in the test set, because this will result in *data leakage* problem. "]},{"cell_type":"code","metadata":{"id":"CqVb22RP9J2I"},"source":["mean_imp_train = SimpleImputer(missing_values=np.nan, strategy='mean')\n","mean_imp_test = SimpleImputer(missing_values=np.nan, strategy='mean')\n","\n","X_train_mean_imp = mean_imp_train.fit_transform(X_train_missing)\n","X_test_mean_imp = mean_imp_test.fit_transform(X_test_missing)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pdFktthy_X01"},"source":["## **Regression Based Imputation**\n","\n","Here, we will now do a regression based imputation on the dataset with missing values. Remember that a regression based imputation uses the other columns of non-missing data to predict the missing data of a given column. "]},{"cell_type":"code","metadata":{"id":"nYuRpsGZ_W6_"},"source":["reg_imp_train = IterativeImputer(missing_values=np.nan)\n","reg_imp_test = IterativeImputer(missing_values=np.nan)\n","\n","X_train_reg_imp = reg_imp_train.fit_transform(X_train_missing)\n","X_test_reg_imp = reg_imp_test.fit_transform(X_test_missing)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cN7oqo0XlFFI"},"source":["## *Question 1*\n"]},{"cell_type":"code","source":["#@markdown Comparing these two imputation methods, which do you think will perform better? Why?\n","ans1 = 'Regression Based Imputation will perform better. As it makes use of other columns to continuously make a prediction of the missing values, it is expected to be more accurate in comparison to blindly substituting the mean of the feature values.' #@param {type:\"string\"}"],"metadata":{"cellView":"form","id":"P3rTA2fcIdI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grader.grade(test_case_id = 'test_imputation', answer = ans1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BRifo6ybIpla","executionInfo":{"status":"ok","timestamp":1668386581988,"user_tz":300,"elapsed":482,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"362e7f3e-5f0a-410b-c27c-223dbe674b52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct! You earned 1.0/1.0 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n"]}]},{"cell_type":"markdown","metadata":{"id":"P9RdiNKYApnZ"},"source":["## **Training and Testing on the Imputed Datasets**\n","\n","Now, we can verify your hypothesis through experiment. Let's train OLS regression on each of the imputed data sets and compare their testing MSE. \n","\n"]},{"cell_type":"code","metadata":{"id":"inMdjglMA3S1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668386589246,"user_tz":300,"elapsed":162,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"3f2f5268-6afa-43de-c883-97a7f3fc2b9e"},"source":["# training on mean imputed data\n","mean_imp_lm = LinearRegression().fit(X_train_mean_imp, y_train)\n","y_pred_mean_imp = mean_imp_lm.predict(X_test_mean_imp)\n","mse_mean_imp = mean_squared_error(y_test, y_pred_mean_imp)\n","\n","print(\"Mean Imputed Data MSE: \" + str(mse_mean_imp))\n","\n","# training on regression imputed data\n","reg_imp_lm = LinearRegression().fit(X_train_reg_imp, y_train)\n","y_pred_reg_imp = reg_imp_lm.predict(X_test_reg_imp)\n","mse_reg_imp = mean_squared_error(y_test, y_pred_reg_imp)\n","\n","print(\"Regression Imputed Data MSE: \" + str(mse_reg_imp))\n","\n","# training on full X_train data\n","lm = LinearRegression().fit(X_train, y_train)\n","y_pred = lm.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","\n","print(\"True X_train MSE: \" + str(mse))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Imputed Data MSE: 1039.0120582518593\n","Regression Imputed Data MSE: 1039.0904507534844\n","True X_train MSE: 2.5145436839611363e-26\n"]}]},{"cell_type":"markdown","metadata":{"id":"1zvqZLuhFVim"},"source":["## *Question 2*\n","\n","Observe the above result, answer the following questions:\n"]},{"cell_type":"code","metadata":{"id":"TF-iLwFuogSM","cellView":"form"},"source":["#@markdown Which imputation technique produced the lower MSE? Why do you think that this is the case? Is this what you expected?\n","ans2 = 'Mean imputed produced lower MSE. It is not what I initially expected but MSE is not really the metric I was approaching the problem first. The mean produced lower MSE as the mean substitutes the mean of all values in the feature column, hence effectively reducing the variance and hence the MSE of the data.' #@param {type:\"string\"}\n","\n","#@markdown Think about why regression imputation didn't work significantly better here despite having \"more information\"? (hint: the x variables of the data generating model are indepedent)\n","ans3 = 'The variables are independent of each other and hence have no real correlation. This leads to more error in the prediction that accumulates over the number of features of predicted.' #@param {type:\"string\"}\n","\n","#@markdown How might these MSE results change if there was a slight correlation between the x variables?\n","ans4 = 'Regression imputation will work better' #@param {type:\"string\"}\n","\n","#@markdown Which imputation technique do you think would work better in the real world? Why?\n","ans5 = 'In the real world, regression imputation will work better but requires more computation for larger datasets. Hence mean imputation is a popular choice.' #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grader.grade(test_case_id = 'test_observations', answer = [ans2, ans3, ans4, ans5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ags_P8RuJuxr","executionInfo":{"status":"ok","timestamp":1668386844422,"user_tz":300,"elapsed":668,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"9d7bd25f-0e0b-45cd-a1ca-7c344266873d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct! You earned 4.0/4.0 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n"]}]}]}