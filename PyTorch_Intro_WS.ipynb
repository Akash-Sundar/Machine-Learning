{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AGiPrn4G6-vUuECk5FVE5j4uG-vZQq5k","timestamp":1666465593881}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hGrwUfxhcZCe"},"source":["# **CIS 520: Machine Learning**\n","\n","## **PyTorch Basics**\n","\n","\n","- **Content Creators:** Mihir Parmar, Tejas Srivastava, Siyun Hu\n","- **Content Reviewers:**  Shaozhe Lyu, Michael Zhou, Ani Cowlagi\n","- **Objective:** In this tutorial, we will cover:\n","\n","    -  [PyTorch](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html)\n","    - [Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py): Automatic Differentiation \n","\n","- **Acknowledge:**\n","    - [Pytorch official tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).<br>\n","\n","    - [CIS 522 Spring '20](https://www.seas.upenn.edu/~cis522/index.html) \n","\n","    - CIS 680 Spring 2020 Pytorch Tutorial\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iVDsklx0M0Rf"},"source":["\n","\n"," \n","**Note**: Remember to change Runtime type to GPU Hardware accelerated for leveraging GPU resources.\n","```\n","> Runtime > Change runtime type > Hardware accelerator > Select GPU\n","```"]},{"cell_type":"markdown","metadata":{"id":"Tn3wuswcj3fw"},"source":["## **Autograding and the PennGrader**\n","\n","\n","Enter your PennID (numbers not letters!) in the specified section."]},{"cell_type":"markdown","source":["### Imports and Setup (Do Not Modify This Section)"],"metadata":{"id":"fdSYsUbBW-Yr"}},{"cell_type":"code","metadata":{"id":"AFixFKy2kAev","executionInfo":{"status":"ok","timestamp":1666729047708,"user_tz":240,"elapsed":3135,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["%%capture\n","!pip install penngrader\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import random \n","import numpy as np\n","import pandas as pd\n","import os\n","import sys\n","import matplotlib.pyplot as plt\n","from numpy.linalg import *\n","\n","import dill\n","import base64"],"metadata":{"id":"HFBgYui-WnCl","executionInfo":{"status":"ok","timestamp":1666729047960,"user_tz":240,"elapsed":259,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# For autograder only, do not modify this cell. \n","# True for Google Colab, False for autograder\n","NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n","if NOTEBOOK:\n","    print(\"[INFO, OK] Google Colab.\")\n","else:\n","    print(\"[INFO, OK] Autograder.\")\n","    sys.exit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wql2sB6dW6x-","executionInfo":{"status":"ok","timestamp":1666729047962,"user_tz":240,"elapsed":35,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"0ec7ec6b-52af-4e20-d616-76ebf3bde8d3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO, OK] Google Colab.\n"]}]},{"cell_type":"markdown","source":["### Insert PennID here!"],"metadata":{"id":"3f1c5R-ZXCwC"}},{"cell_type":"code","metadata":{"id":"kcS79C9blJBm","executionInfo":{"status":"ok","timestamp":1666729047963,"user_tz":240,"elapsed":27,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n","#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n","STUDENT_ID = 57931095 # YOUR PENN-ID GOES HERE AS AN INTEGER#"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"wd0oMe8glNnA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666729047965,"user_tz":240,"elapsed":27,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"e23905f3-03d1-4b16-d370-2b1f0c1e8686"},"source":["import penngrader.grader\n","\n","grader = penngrader.grader.PennGrader(homework_id = 'CIS_5200_202230_HW_PyTorch_Intro_WS', student_id = STUDENT_ID)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["PennGrader initialized with Student ID: 57931095\n","\n","Make sure this correct or we will not be able to store your grade\n"]}]},{"cell_type":"code","source":["# A helper function for grading utils\n","def grader_serialize(obj):        # A helper function\n","    '''Dill serializes Python object into a UTF-8 string'''\n","    byte_serialized = dill.dumps(obj, recurse = True)\n","    return base64.b64encode(byte_serialized).decode(\"utf-8\")"],"metadata":{"id":"oxdWoARsXS-0","executionInfo":{"status":"ok","timestamp":1666729047968,"user_tz":240,"elapsed":19,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYYGhYnW_LLX"},"source":["\n","## **What is PyTorch?**\n","\n","\n","It’s a Python-based scientific computing package targeted at two sets of\n","audiences:\n","\n","-  A replacement for NumPy to use the power of GPUs\n","-  A deep learning platform that provides maximum flexibility\n","   and speed\n","\n","At its core, PyTorch provides a few key features:\n","\n","- A multidimensional **Tensor** object, similar to [numpy](https://numpy.org/) but with GPU acceleration.\n","- An optimized **autograd** engine for automatically computing derivatives\n","- A clean, modular API for building and deploying **deep learning models**\n","\n","You can find more information about PyTorch by following one of the [official tutorials](https://pytorch.org/tutorials/) or by [reading the documentation](https://pytorch.org/docs/1.1.0/).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5v7DiUL1reup"},"source":["## **Getting Started**\n","\n","## Tensors\n","\n","\n","\n","Tensors are similar to NumPy’s ndarrays."]},{"cell_type":"markdown","metadata":{"id":"ZteB6Byur1Yz"},"source":["### Tensor initialization"]},{"cell_type":"code","metadata":{"id":"_nuxuebR_LLY","executionInfo":{"status":"ok","timestamp":1666729050195,"user_tz":240,"elapsed":2245,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["import torch"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F1sXjie2_LLa"},"source":["Construct a 5x3 matrix, uninitialized:\n","\n"]},{"cell_type":"code","metadata":{"id":"owPTQ-_i_LLc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb68eda6-92b7-42ae-a42e-ab4ee439f87e","executionInfo":{"status":"ok","timestamp":1666729050498,"user_tz":240,"elapsed":312,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = torch.empty(5, 3)\n","print(x)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[5.2175e-35, 0.0000e+00, 3.3631e-44],\n","        [0.0000e+00,        nan, 0.0000e+00],\n","        [1.1578e+27, 1.1362e+30, 7.1547e+22],\n","        [4.5828e+30, 1.2121e+04, 7.1846e+22],\n","        [9.2198e-39, 7.0374e+22, 0.0000e+00]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"wSm5Q3Kk_LLe"},"source":["Construct a randomly initialized matrix:\n","\n"]},{"cell_type":"code","metadata":{"id":"YUSbJtCp_LLf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e194ff7-de1f-47f0-8c10-9e56eea876cb","executionInfo":{"status":"ok","timestamp":1666729050499,"user_tz":240,"elapsed":69,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = torch.rand(5, 3)\n","print(x)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.7046, 0.4561, 0.3029],\n","        [0.1597, 0.9120, 0.2677],\n","        [0.3651, 0.5455, 0.2029],\n","        [0.1126, 0.5347, 0.0976],\n","        [0.1275, 0.0554, 0.8796]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"KtmvilDg_LLh"},"source":["Construct a matrix filled with zeros and of dtype long:\n","\n"]},{"cell_type":"code","metadata":{"id":"2nKUNNBA_LLh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"14b42763-f7db-4566-c23a-04dc54c2f12d","executionInfo":{"status":"ok","timestamp":1666729050499,"user_tz":240,"elapsed":66,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = torch.zeros(5, 3, dtype=torch.long)\n","print(x)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"bDxsatUX_LLj"},"source":["Construct a tensor directly from data:\n","\n"]},{"cell_type":"code","metadata":{"id":"1dGnsDNx_LLk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba505cc5-16b7-4816-daf8-b5e1af557ccf","executionInfo":{"status":"ok","timestamp":1666729050500,"user_tz":240,"elapsed":64,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = torch.tensor([5.5, 3])\n","print(x)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([5.5000, 3.0000])\n"]}]},{"cell_type":"markdown","metadata":{"id":"BB75YoSV_LLl"},"source":["Create a tensor based on an existing tensor. These methods  \n","will reuse properties of the input tensor, e.g. dtype, unless  \n","new values are provided by the user.\n","\n"]},{"cell_type":"code","metadata":{"id":"u0RaDsNl_LLm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"731477eb-7648-46a1-ac4e-638b5dd1a82a","executionInfo":{"status":"ok","timestamp":1666729050500,"user_tz":240,"elapsed":61,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n","print(x)\n","\n","x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n","print(x)                                      # result has the same size"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]], dtype=torch.float64)\n","tensor([[-1.8300, -0.0742, -1.0646],\n","        [ 0.1174,  0.3832,  0.1276],\n","        [-0.9881,  1.5877,  0.8806],\n","        [ 0.5618,  2.7774, -0.3667],\n","        [ 0.8197,  0.2053, -0.5026]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"oZ6a_Gq__LLo"},"source":["Get its size.   \n","**Note**: torch.Size is a tuple, so it supports all tuple operations.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"WZ2fQtXh_LLp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a45e2d0-a010-4334-e4e7-4830580a36d6","executionInfo":{"status":"ok","timestamp":1666729050501,"user_tz":240,"elapsed":58,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["print(x.size())"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 3])\n"]}]},{"cell_type":"markdown","metadata":{"id":"NA_t2w7U_LLs"},"source":["### Tensor Operations\n","There are multiple syntaxes for operations. In the following example, we will take a look at the addition operation.\n","\n","Addition: Syntax 1\n","\n"]},{"cell_type":"code","metadata":{"id":"7-LSd4Vr_LLs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a56a12d1-f5a6-49fa-e249-f471f7ddf45f","executionInfo":{"status":"ok","timestamp":1666729050501,"user_tz":240,"elapsed":54,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["y = torch.rand(5, 3)\n","print(x + y)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.8949,  0.6923, -0.9801],\n","        [ 0.4427,  0.9160,  0.3020],\n","        [-0.8011,  2.3191,  1.6725],\n","        [ 0.7383,  3.2011,  0.4783],\n","        [ 0.8303,  0.4120, -0.1129]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"3HwzDYtB_LLu"},"source":["Addition: Syntax 2\n","\n"]},{"cell_type":"code","metadata":{"id":"-HNKhMig_LLv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b4fab191-9227-4fad-b367-8846ea6fc772","executionInfo":{"status":"ok","timestamp":1666729050502,"user_tz":240,"elapsed":52,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["print(torch.add(x, y))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.8949,  0.6923, -0.9801],\n","        [ 0.4427,  0.9160,  0.3020],\n","        [-0.8011,  2.3191,  1.6725],\n","        [ 0.7383,  3.2011,  0.4783],\n","        [ 0.8303,  0.4120, -0.1129]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ItXUY7HN_LLw"},"source":["Addition: Providing an output tensor as argument\n","\n"]},{"cell_type":"code","metadata":{"id":"uOepVPIM_LLx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1a6eea0-15f9-4076-846a-377b4a8c0caf","executionInfo":{"status":"ok","timestamp":1666729050502,"user_tz":240,"elapsed":39,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["result = torch.empty(5, 3)\n","torch.add(x, y, out=result)\n","print(result)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.8949,  0.6923, -0.9801],\n","        [ 0.4427,  0.9160,  0.3020],\n","        [-0.8011,  2.3191,  1.6725],\n","        [ 0.7383,  3.2011,  0.4783],\n","        [ 0.8303,  0.4120, -0.1129]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"QlCVqVxI_LLy"},"source":["Addition: In-place\n","\n"]},{"cell_type":"code","metadata":{"id":"zlrpqMPT_LLz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f65628f5-f8c1-42db-e06f-724f1faf9a6b","executionInfo":{"status":"ok","timestamp":1666729050503,"user_tz":240,"elapsed":37,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["# adds x to y\n","y.add_(x) # _ at end means the operation mutates tensor y in-place\n","print(y)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.8949,  0.6923, -0.9801],\n","        [ 0.4427,  0.9160,  0.3020],\n","        [-0.8011,  2.3191,  1.6725],\n","        [ 0.7383,  3.2011,  0.4783],\n","        [ 0.8303,  0.4120, -0.1129]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"5tqzs2m8_LL0"},"source":["Besides addition, there are 100+ Tensor operations, including transposing, indexing, slicing,  mathematical operations, linear algebra, random numbers, etc.. There are described [here](http://pytorch.org/docs/torch).\n","\n","You can use standard NumPy-like indexing with all bells and whistles!\n","\n"]},{"cell_type":"code","metadata":{"id":"3h8VYw6L_LL1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"592a4433-d520-4280-a034-ce7d056adea1","executionInfo":{"status":"ok","timestamp":1666729050503,"user_tz":240,"elapsed":34,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["print(x[:, 1])"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-0.0742,  0.3832,  1.5877,  2.7774,  0.2053])\n"]}]},{"cell_type":"markdown","metadata":{"id":"XyhR2UZj_LL2"},"source":["If you want to resize/reshape a tensor, you can use ``torch.view``:\n","\n"]},{"cell_type":"code","metadata":{"id":"w48nEEw5_LL3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c4e8e32-44e5-4bab-d35a-e48a1e4d8346","executionInfo":{"status":"ok","timestamp":1666729050504,"user_tz":240,"elapsed":32,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = torch.randn(4, 4)\n","y = x.view(16)\n","z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n","print(x.size(), y.size(), z.size())"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"]}]},{"cell_type":"markdown","metadata":{"id":"oJ_AMQn5_LL4"},"source":["If you have a one element tensor, use ``.item()`` to get the value as a\n","Python number:\n","\n"]},{"cell_type":"code","metadata":{"id":"ErxUZdvw_LL5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4308a698-5082-470e-fc7a-01b3b12a254a","executionInfo":{"status":"ok","timestamp":1666729050504,"user_tz":240,"elapsed":28,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = torch.randn(1)\n","print(x)\n","print(x.item())"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.0918])\n","1.0917507410049438\n"]}]},{"cell_type":"markdown","metadata":{"id":"ntxyryJH_LL6"},"source":["\n","\n","## NumPy Bridge\n","\n","\n","Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n","\n","The Torch Tensor and NumPy array will share their underlying memory locations, and changing one will change the other.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cOBGTJEutuyo"},"source":["### Converting a Torch Tensor to a NumPy Array"]},{"cell_type":"code","metadata":{"id":"Sdu-iyie_LL7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ab4a885-655e-46cd-e93c-afdf78ed63b2","executionInfo":{"status":"ok","timestamp":1666729050505,"user_tz":240,"elapsed":25,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["a = torch.ones(5)\n","print(a)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1., 1.])\n"]}]},{"cell_type":"code","metadata":{"id":"p6LVs0qn_LL-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae483371-59d7-494e-a849-3d0f4889418b","executionInfo":{"status":"ok","timestamp":1666729050505,"user_tz":240,"elapsed":22,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["b = a.numpy()\n","print(b)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 1. 1. 1. 1.]\n"]}]},{"cell_type":"markdown","metadata":{"id":"E6vCGNqw_LMA"},"source":["See how the numpy array changed in value.\n","\n"]},{"cell_type":"code","metadata":{"id":"FKzjYrMP_LMB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c6ad3885-fb50-44b9-fc3b-6169e6162e64","executionInfo":{"status":"ok","timestamp":1666729050505,"user_tz":240,"elapsed":19,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["a.add_(1)\n","print(a)\n","print(b)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2., 2., 2., 2., 2.])\n","[2. 2. 2. 2. 2.]\n"]}]},{"cell_type":"markdown","metadata":{"id":"GJ9lW0_n_LMC"},"source":["###Converting NumPy Array to Torch Tensor\n","See how changing the np array changed the Torch Tensor automatically:\n","\n"]},{"cell_type":"code","metadata":{"id":"gzUkmVlH_LMD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d231d8d-5a60-4bfd-d564-93ede964ec60","executionInfo":{"status":"ok","timestamp":1666729050506,"user_tz":240,"elapsed":17,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["import numpy as np\n","a = np.ones(5)\n","b = torch.from_numpy(a)\n","np.add(a, 1, out=a)\n","print(a)\n","print(b)"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[2. 2. 2. 2. 2.]\n","tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"M_d52GZC_LME"},"source":["All the Tensors on the CPU except a CharTensor support conversion to\n","NumPy and back.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"psEZRUOKt88y"},"source":["\n","## CUDA Tensors\n","\n","\n","One of the most important features of PyTorch is that it can use graphics processing units (GPUs) to accelerate its tensor operations.\n","\n","We can easily check whether PyTorch is configured to use GPUs. Tensors can be moved onto any device using the .to() method.\n"]},{"cell_type":"code","metadata":{"id":"cm24dXBqWsmX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5f5faa1-76f6-4cba-96f8-9933b3917a21","executionInfo":{"status":"ok","timestamp":1666729051183,"user_tz":240,"elapsed":691,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["import torch\n","\n","if torch.cuda.is_available:\n","  print('GPU is available for use')\n","else:\n","  print('Cannot use GPU.')"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available for use\n"]}]},{"cell_type":"code","metadata":{"id":"a_MuYscD_LMF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8dc836fe-25e9-4c16-f20f-e1767ce99cd7","executionInfo":{"status":"ok","timestamp":1666729056181,"user_tz":240,"elapsed":5002,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["# let us run this cell only if CUDA is available\n","# We will use ``torch.device`` objects to move tensors in and out of GPU\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")          # a CUDA device object\n","    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n","    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n","    z = x + y\n","    print(z)\n","    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2.0918], device='cuda:0')\n","tensor([2.0918], dtype=torch.float64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"wFKxZOO5uZEe"},"source":["If the device is cuda, you can use `.cuda()` to convert tensor onto cuda."]},{"cell_type":"markdown","metadata":{"id":"0uAykVi7_SMn"},"source":["\n","## Autograd: Automatic Differentiation\n","\n","\n","Central to all neural networks in PyTorch is the ``autograd`` package. To learn more about ``autograd`` and ``Function`` in PyTorch, see [Documentation](http://pytorch.org/docs/autograd).\n","\n","Let’s first briefly visit this. We will train our first neural network in another Worksheet.\n","\n","\n","The ``autograd`` package provides automatic differentiation for all operations\n","on Tensors. It is a define-by-run framework, which means that your backprop is\n","defined by how your code is run, and that every single iteration can be\n","different.\n","\n","Let us see this in more simple terms with some examples.\n","\n","### Tensor\n","\n","\n","``torch.Tensor`` is the central class of the package. If you set its attribute\n","``.requires_grad`` as ``True``, it starts to track all operations on it. When  \n","you finish your computation you can call ``.backward()`` and have all the\n","gradients computed automatically. The gradient for this tensor will be accumulated into ``.grad`` attribute.\n","\n","To stop a tensor from tracking history, you can call ``.detach()`` to detach\n","it from the computation history, and to prevent future computation from being\n","tracked.\n","\n","To prevent tracking history (and use of memory), you can also wrap the code block\n","in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n","model because the model may have trainable parameters with `requires_grad=True`,\n","but for which we don't need the gradients.\n","\n","There’s one more class which is very important for autograd implementation - a ``Function``.  \n","\n","<img src=\"https://miro.medium.com/max/1536/1*wE1f2i7L8QRw8iuVx5mOpw.png\" alt=\"Function\" width=\"600\"/>\n","\n","``Tensor`` and ``Function`` are interconnected and build up an acyclic\n","graph, that encodes a complete history of computation. Each tensor has  \n","a ``.grad_fn`` attribute that references a ``Function`` that has created\n","the ``Tensor`` (except for Tensors created by the user - their  \n","``grad_fn is None``).\n","\n","<img src=\"https://miro.medium.com/max/1684/1*FDL9Se9otGzz83F3rofQuA.png\" alt=\"Computation Graph\" width=\"500\"/>\n","\n","If you want to compute the derivatives, you can call ``.backward()`` on\n","a ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element  \n","data), you don’t need to specify any arguments to ``backward()``,\n","however if it has more elements, you need to specify a ``gradient``  \n","argument that is a tensor of matching shape.\n","\n","<img src=\"https://miro.medium.com/max/1684/1*EWpoG5KayZSqkWmwM_wMFQ.png\" alt=\"Computation Graph with Gradients\" width=\"500\"/>"]},{"cell_type":"code","metadata":{"id":"6BPRyvT__SMo","executionInfo":{"status":"ok","timestamp":1666729056182,"user_tz":240,"elapsed":45,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["import torch"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WOV1ONxp_SMp"},"source":["Create a tensor and set requires_grad=True to track computation with it:\n","\n"]},{"cell_type":"code","metadata":{"id":"oy4yu9BO_SMq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a1639ce-c056-4b0a-ed7c-777ff8c0a0b4","executionInfo":{"status":"ok","timestamp":1666729056183,"user_tz":240,"elapsed":43,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n"]}]},{"cell_type":"markdown","metadata":{"id":"CFGCV-Ku_SMr"},"source":["Do an operation of tensor:\n","\n"]},{"cell_type":"code","metadata":{"id":"Ly8h4Eue_SMs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b29a4e6-aba9-47fb-f472-0dda7a865943","executionInfo":{"status":"ok","timestamp":1666729056184,"user_tz":240,"elapsed":37,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["y = x + 2\n","print(y)"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"QseU9J4u_SMt"},"source":["``y`` was created as a result of an operation, so it has a ``grad_fn``.\n","\n"]},{"cell_type":"code","metadata":{"id":"qdW2vLpD_SMu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8ca164a-0169-48b5-ec96-95036f4b3334","executionInfo":{"status":"ok","timestamp":1666729056185,"user_tz":240,"elapsed":32,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["print(y.grad_fn)"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["<AddBackward0 object at 0x7f8488800850>\n"]}]},{"cell_type":"markdown","metadata":{"id":"tLrPxNEL_SMw"},"source":["Do more operations on y:\n","\n"]},{"cell_type":"code","metadata":{"id":"t7OT_PJF_SMw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bbff8299-2338-42c0-9e9f-14d83c42e456","executionInfo":{"status":"ok","timestamp":1666729056187,"user_tz":240,"elapsed":28,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"RwQqBYkl_SMy"},"source":["``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n","flag in-place. The input flag defaults to ``False`` if not given.\n","\n"]},{"cell_type":"code","metadata":{"id":"UmLT9jQW_SMy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1de6de49-8308-42bb-cb80-9585fc21408a","executionInfo":{"status":"ok","timestamp":1666729056188,"user_tz":240,"elapsed":23,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["a = torch.randn(2, 2)\n","a = ((a * 3) / (a - 1))\n","print(a.requires_grad)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.grad_fn)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","True\n","<SumBackward0 object at 0x7f84f63ed950>\n"]}]},{"cell_type":"markdown","metadata":{"id":"_IvjQxgM5CTr"},"source":["We can visualize the computational graph saved by autograd using torchviz's make_dot()"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":755},"id":"1uP5PACQ4apq","outputId":"d8e4860d-2c0f-4398-b843-2fc57b0accd8","executionInfo":{"status":"ok","timestamp":1666729061761,"user_tz":240,"elapsed":5591,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["!pip install torchviz\n","from torchviz import make_dot\n","make_dot(out) "],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchviz\n","  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.12.1+cu113)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.1.1)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=798f4dece387271d952b7467593c3abfbf54b37d19f4b2efb8f457e9e67bc908\n","  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n","Successfully built torchviz\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.2\n"]},{"output_type":"execute_result","data":{"text/plain":["<graphviz.dot.Digraph at 0x7f84fcb9b410>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"109pt\" height=\"381pt\"\n viewBox=\"0.00 0.00 109.00 381.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 377)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-377 105,-377 105,4 -4,4\"/>\n<!-- 140209041044496 -->\n<g id=\"node1\" class=\"node\">\n<title>140209041044496</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140209152360656 -->\n<g id=\"node2\" class=\"node\">\n<title>140209152360656</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"98,-86 3,-86 3,-67 98,-67 98,-86\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 140209152360656&#45;&gt;140209041044496 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140209152360656&#45;&gt;140209041044496</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-66.9688C50.5,-60.1289 50.5,-50.5621 50.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-41.3678 50.5,-31.3678 47.0001,-41.3678 54.0001,-41.3678\"/>\n</g>\n<!-- 140209152361744 -->\n<g id=\"node3\" class=\"node\">\n<title>140209152361744</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-141 6,-141 6,-122 95,-122 95,-141\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140209152361744&#45;&gt;140209152360656 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140209152361744&#45;&gt;140209152360656</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-121.9197C50.5,-114.9083 50.5,-105.1442 50.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-96.3408 50.5,-86.3408 47.0001,-96.3409 54.0001,-96.3408\"/>\n</g>\n<!-- 140209152363600 -->\n<g id=\"node4\" class=\"node\">\n<title>140209152363600</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-196 6,-196 6,-177 95,-177 95,-196\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140209152363600&#45;&gt;140209152361744 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140209152363600&#45;&gt;140209152361744</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-176.9197C50.5,-169.9083 50.5,-160.1442 50.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-151.3408 50.5,-141.3408 47.0001,-151.3409 54.0001,-151.3408\"/>\n</g>\n<!-- 140209152363984 -->\n<g id=\"node5\" class=\"node\">\n<title>140209152363984</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-251 6,-251 6,-232 95,-232 95,-251\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140209152363984&#45;&gt;140209152363600 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140209152363984&#45;&gt;140209152363600</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M45.2332,-231.9197C43.8546,-224.9083 43.4371,-215.1442 43.9804,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47.479,-206.6838 45.1826,-196.3408 40.5278,-205.8583 47.479,-206.6838\"/>\n</g>\n<!-- 140209152363984&#45;&gt;140209152363600 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140209152363984&#45;&gt;140209152363600</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M55.7668,-231.9197C57.1454,-224.9083 57.5629,-215.1442 57.0196,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"60.4722,-205.8583 55.8174,-196.3408 53.521,-206.6838 60.4722,-205.8583\"/>\n</g>\n<!-- 140209152361872 -->\n<g id=\"node6\" class=\"node\">\n<title>140209152361872</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-306 0,-306 0,-287 101,-287 101,-306\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140209152361872&#45;&gt;140209152363984 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140209152361872&#45;&gt;140209152363984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-286.9197C50.5,-279.9083 50.5,-270.1442 50.5,-261.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-261.3408 50.5,-251.3408 47.0001,-261.3409 54.0001,-261.3408\"/>\n</g>\n<!-- 140207202748944 -->\n<g id=\"node7\" class=\"node\">\n<title>140207202748944</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-373 21,-373 21,-342 80,-342 80,-373\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n<!-- 140207202748944&#45;&gt;140209152361872 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140207202748944&#45;&gt;140209152361872</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-341.791C50.5,-334.0249 50.5,-324.5706 50.5,-316.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-316.0647 50.5,-306.0648 47.0001,-316.0648 54.0001,-316.0647\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"xIOCut2q_SM0"},"source":["### Gradients\n","\n","Let's do backprop now.\n","Because ``out`` contains a single scalar, ``out.backward()`` is\n","equivalent to ``out.backward(torch.tensor(1))``.\n","\n"]},{"cell_type":"code","metadata":{"id":"cirFNii__SM1","executionInfo":{"status":"ok","timestamp":1666729061763,"user_tz":240,"elapsed":18,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["out.backward()"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJD3QRXu_SM3"},"source":["Use ```x.grad``` to get the gradient of ```out``` wrt ```x``` and convert it to a numpy array.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"owYJV2YJ_SM5","executionInfo":{"status":"ok","timestamp":1666729061764,"user_tz":240,"elapsed":17,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x_grad_np = x.grad.numpy() # Get the gradient dout / dx here and convert it to numpy"],"execution_count":36,"outputs":[]},{"cell_type":"code","source":["print(x_grad_np)"],"metadata":{"id":"ING4j8zgwE8f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666729061765,"user_tz":240,"elapsed":17,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"69b5f3f3-317a-4387-c4a9-d0b4c559de52"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["[[4.5 4.5]\n"," [4.5 4.5]]\n"]}]},{"cell_type":"code","source":["grader.grade(test_case_id = 'test_x_grad_np', answer = x_grad_np)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PiJseWz-wamq","executionInfo":{"status":"ok","timestamp":1666729061969,"user_tz":240,"elapsed":218,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"8a6f3ef2-3fb1-4ecf-974a-24399d8a8b1a"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct! You earned 3.0/3.0 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n"]}]},{"cell_type":"markdown","metadata":{"id":"DtoxVdoh_SM7"},"source":["You should have gotten a matrix of ``4.5``. Let’s call the ``out``\n","*Tensor* “$o$”.  \n","We have that $o = \\frac{1}{4}\\sum_i z_i$, $z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$. Therefore, $\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0XRYgvUP_SM8"},"source":["You can do many crazy things with autograd!\n","\n"]},{"cell_type":"code","metadata":{"id":"epiw0-Jg_SM8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"37c9862b-2b52-471f-c047-c999ea22a7b7","executionInfo":{"status":"ok","timestamp":1666729061970,"user_tz":240,"elapsed":26,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","while y.data.norm() < 1000:\n","    y = y * 2\n","\n","print(y)"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([  980.1608, -1653.0077,   336.4561], grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"code","metadata":{"id":"KQQqn3nc_SM-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b578da2f-5f46-4841-8f66-ac998bbd43f3","executionInfo":{"status":"ok","timestamp":1666729061971,"user_tz":240,"elapsed":19,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}}},"source":["gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","y.backward(gradients)\n","\n","print(x.grad)"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"]}]},{"cell_type":"markdown","metadata":{"id":"FQNia9mB_SNB"},"source":["You can also stop autograd from tracking history on Tensors\n","with ``.requires_grad=True`` by wrapping the code block in\n","``with torch.no_grad()``. Compute the the Frobenius norm of the quantity ```x**2```, then call backward on it. Compute the same quantity in a ``torch.no_grad()`` environment.\n"]},{"cell_type":"code","metadata":{"id":"7b0W4gN0_SNB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666729100684,"user_tz":240,"elapsed":156,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"fed9631f-45b8-4f3f-fd7d-5ae0fa2d42a2"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x.grad)\n","\n","x_squared_frob_grad = torch.norm(x**2, p='fro') # Compute the Frobenius norm of x**2\n","x_squared_frob_grad.backward()  # Call backward on x_squared_frob_grad\n","\n","print(x.grad) \n","\n","with torch.no_grad():\n","  x_squared_frob_no_grad =  torch.norm(x**2, p='fro') # Compute the Frobenius norm of x**2 in a torch.no_grad() environment\n"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n","tensor([[1., 1.],\n","        [1., 1.]])\n"]}]},{"cell_type":"code","source":["grader.grade(test_case_id = 'test_torch_no_grad', answer = (x.grad.numpy(), x_squared_frob_no_grad.grad_fn))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtxFXHKQzlvb","executionInfo":{"status":"ok","timestamp":1666729104060,"user_tz":240,"elapsed":345,"user":{"displayName":"Akash Sundar","userId":"17827216673145327670"}},"outputId":"b2b513d4-6db0-4cb6-800e-23659f57d5cd"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct! You earned 3.0/3.0 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n"]}]},{"cell_type":"markdown","metadata":{"id":"TA5V-fCI6pEe"},"source":["## Model in PyTorch\n","\n","PyTorch offers a unified way to define a model: extends the existing Pytorch Module. \n","\n","```python\n","class Model(torch.nn.Module):\n","    def __init__(self, var):\n","        super().__init__()\n","        self.var = var\n","        \n","        self.length = xxx\n","    def count_number(self,var):\n","      return xxx\n","    def forward(self, x):\n","        out = f(x)\n","        x = self.length\n","        y = self.count_number(self.var)\n","        return out\n","```\n","\n"," There are two functions to overload:\n"," 1. \\_\\_init\\_\\_ constructor function. First we need to initialize torch.nn.Module with super().\\_\\_init\\_\\_(). Then we initialize a few variables we need.\n"," 2. forward function. We define the computation model in the forward function, and the gradient computation will be taken care of by the PyTorch.\n","\n","\n"," PyTorch has built-in support for common neural network layers. Usually we define them in the \\_\\_init\\_\\_ function and then call them in the forward function. For example,\n","\n"," ```python\n","class Model(torch.nn.Module):\n","    def __init__(self, var):\n","        super().__init__()\n","        self.fc1 = torch.nn.Linear(in_dim, out_dim1)\n","        self.fc2 = torch.nn.Linear(out_dim1, out_dim2)\n","\n","    def forward(self, x):\n","        out = self.fc2(self.fc1(x))\n","        return out\n","```\n","\n","For consecutive layers, we can also use torch.nn.Sequential to chain them together\n"," ```python\n","class Model(torch.nn.Module):\n","    def __init__(self, var):\n","        super().__init__()\n","        self.net = torch.Sequential(\n","            torch.nn.Linear(in_dim, out_dim1),\n","            torch.nn.Linear(out_dim1, out_dim2)\n","        )\n","\n","\n","    def forward(self, x):\n","        out = self.net(x)\n","        return out\n","```"]},{"cell_type":"markdown","metadata":{"id":"AjSwlbcy7Bt8"},"source":["### Loss function\n","\n","PyTorch also has built-in support for common loss functions. \n","1. torch.nn.L1Loss\n","2. torch.nn.MSELoss\n","3. torch.nn.CrossEntropyLoss\n","\n","The typical usage is\n","```python\n","# criterion is a function\n","criterion = torch.nn.XXLoss()\n","# compute the loss\n","loss = criterion(pred, target)\n","# later perform backward pass on loss\n","loss.backward()\n","```"]},{"cell_type":"markdown","metadata":{"id":"DL0aZwOq7YX7"},"source":["### Optimizer\n","\n","We use gradient descent as the optimization algorithm to minimize the loss function. There are also other advanced optimization algorithms. PyTorch wraps up these algorithms in the optimizer class. \n","To create an optimizer, we need to tell it what to optimize: typically we pass in parameters of the model we defined earlier. We can also specify the learning rate. \n"," ```python\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n"," ```\n"," Once we have the optimizer, the update step \n"," ```\n","fc.weight -= dweight * lr\n","fc.bias -= dbias * lr\n"," ```\n"," becomes\n"," ```python\n","optimizer.step()\n"," ```\n"," By default, the gradient of the optimizer will accumulate over time. So in the training loop, we need to set the gradient to be 0 before we execute the update step:\n","```python\n","optimizer.zero_grad()\n","# compute the gradient\n","optimizer.step()\n"," ```\n","\n","You will get a chance to implement such a model and training loop in later worksheets and homeworks!"]},{"cell_type":"markdown","metadata":{"id":"Yk0c2GezbXlx"},"source":["## Submitting to the Autograder"]},{"cell_type":"markdown","metadata":{"id":"M0zY-sbUMCh6"},"source":["First of all, please run your notebook from beginning to end and ensure you are getting all the points from the autograder!"]},{"cell_type":"markdown","metadata":{"id":"14QY2kw0bfhc"},"source":["Now go to the File menu and choose \"Download .ipynb\".  Go to [Gradescope](https://www.gradescope.com/courses/409970) and:\n","\n","1. From \"File\" --> Download both .ipynb and .py files\n","1. Name these files `PyTorch_Intro_WS.ipynb` and `PyTorch_Intro_WS.py` respectively\n","1. Sign in using your Penn email address (if you are a SEAS student we recommend using the Google login) and ensure  your class is \"CIS 5200\"\n","1. Select **Worksheet: PyTorch Intro**\n","1. Upload both files\n","1. PLEASE CHECK THE AUTOGRADER OUTPUT TO ENSURE YOUR SUBMISSION IS PROCESSED CORRECTLY!\n","\n","You should be set! Note that this assignment has 10 autograded points that will show up upon submission. Points are awarded based on a combination of correctness and sufficient effort. "]}]}